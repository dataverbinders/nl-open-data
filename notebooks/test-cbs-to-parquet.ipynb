{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Fetching data from CBS using OData v3 API into parquet format\n",
    "\n",
    "## A fast and convenient way to fetch CBS data\n",
    "\n",
    "- CBS API is well documented. Features that we use for `nl-open-data` are:\n",
    "  - catalog function: quickly search through available datasets\n",
    "  - well-typed datasets, including metadata with definitions of dimensions\n",
    "\n",
    "- We currently use v3, v4 is in beta. Both return paginated results with 10,000 records per page. An `odata.nextLink` is provided at the end of each page (which is null if there are no more pages)\n",
    "- Fetching the pages for a single dataset can take a long time if the dataset is large. Although you could hack your way around it by looking up the total number of rows in the catalog and run processes in parallel, we prefer to keep it simple and sequential.\n",
    "- [cbsodata](https://github.com/J535D165/cbsodata) is an unofficial Python library for using the API. The add an extra processing step by combining the paginated results into a single parquet file, which is suitable for uploading into a blob storage service like Azure Storage, Google Cloud Storage or AWS Simple Cloud Storage (S3)\n",
    "\n",
    "## Converting CBS OData to parquet\n",
    "\n",
    "- json with list should be converted to newline delimited json (ndjson)\n",
    "- ndjson can be read into pyarrow.Table and converted to parquet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import cbsodata\n",
    "import pyarrow.json\n",
    "import pyarrow.parquet\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "TEMP = \"test-cbs-to-parquet\"\n",
    "TEMP_NDJSON = Path(TEMP + \".ndjson\")\n",
    "TEMP_PARQUET = Path(TEMP + \".parquet\")\n",
    "\n",
    "TESTDATA = \"82070ENG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "ID: int64\n",
       "Gender: string\n",
       "PersonalCharacteristics: string\n",
       "CaribbeanNetherlands: string\n",
       "Periods: string\n",
       "EmployedLabourForceInternatDef_1: int64\n",
       "EmployedLabourForceNationalDef_2: int64"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# fetch data and parse into ndjson\n",
    "data = cbsodata.get_data(TESTDATA)\n",
    "with open(TEMP_NDJSON, \"w+\") as ndjson:\n",
    "    for record in data:\n",
    "        ndjson.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "table = pyarrow.json.read_json(TEMP_NDJSON)\n",
    "pyarrow.parquet.write_table(table, TEMP_PARQUET)\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0      2012\n",
       "1      2012\n",
       "2      2012\n",
       "3      2012\n",
       "4      2012\n",
       "       ... \n",
       "310    2012\n",
       "311    2012\n",
       "312    2012\n",
       "313    2012\n",
       "314    2012\n",
       "Name: Periods, Length: 315, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df = pd.read_parquet(TEMP_PARQUET)\n",
    "df.Periods"
   ]
  },
  {
   "source": [
    "## TO DO\n",
    "\n",
    "The `cbsodata` method already flattens the dimensions in the TypedDataSet (see [here in the repo](https://github.com/dataverbinders/cbsodata/blob/4922b102a72f5e1d5c67c06b73d827a338305e66/cbsodata/cbsodata3.py#L488)). In the example dataset, the period '2012 JJ00' is converted to the string \"2012\". \n",
    "\n",
    "We want to keep the original data (we will flatten the dimensions later), hence optimize and remove unneccesary intermediate steps in `cbsodata` methods:\n",
    "- fetching a page returns json --> store this and immediately start downloading `nextLink`\n",
    "- `value` in json contains list of dicts --> use Prefect/Dask to convert into ndjson and concatenate into parquet\n",
    "\n",
    "\n",
    "Example `TypedDataSet`: https://opendata.cbs.nl/ODataApi/OData/37506wwm/TypedDataSet "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_NDJSON.unlink()\n",
    "TEMP_PARQUET.unlink()"
   ]
  }
 ]
}